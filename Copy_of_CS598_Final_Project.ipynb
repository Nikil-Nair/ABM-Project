{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nikil-Nair/ABM-Project/blob/main/Copy_of_CS598_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before you use this template\n",
        "\n",
        "This template is just a recommended template for project Report. It only considers the general type of research in our paper pool. Feel free to edit it to better fit your project. You will iteratively update the same notebook submission for your draft and the final submission. Please check the project rubriks to get a sense of what is expected in the template.\n",
        "\n",
        "---\n",
        "\n",
        "# FAQ and Attentions\n",
        "* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n",
        "* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n",
        "* any report must have run-able codes and necessary annotations (in text and code comments).\n",
        "* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n",
        "must be within 8 min, otherwise, you may get penalty on the grade.\n",
        "  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n",
        "  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n",
        "  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n",
        "* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n",
        "* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n",
        "* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary."
      ],
      "metadata": {
        "id": "j01aH0PR4Sg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Notebook to Google Drive\n",
        "Upload the data, pretrianed model, figures, etc to your Google Drive, then mount this notebook to Google Drive. After that, you can access the resources freely.\n",
        "\n",
        "Instruction: https://colab.research.google.com/notebooks/io.ipynb\n",
        "\n",
        "Example: https://colab.research.google.com/drive/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q\n",
        "\n",
        "Video: https://www.youtube.com/watch?v=zc8g8lGcwQU"
      ],
      "metadata": {
        "id": "dlv6knX04FiY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gI2cQ-kkgHfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Repository: https://github.com/rvinas/GTEx-imputation\n",
        "\n",
        "### Background\n",
        "The premise of the paper [1] we chose is to utilize machine learning models to rebuild a\n",
        "transcriptome when presented with a subset of genes. This type of problem is primarily data processing related to bioinformatics and computational biology. Predicting the whole transcriptome could\n",
        "have significant implications in biological and clinical applications. A\n",
        "transcriptome put simply is the total set of RNA molecules, including messenger RNA (mRNA), present in a cell at a specific time under specific conditions. Through profiling the transcriptome it is possible to uncover disease\n",
        "mechanisms, propose novel drug targets, propose a basis for comparative\n",
        "genomics etc. Similarly acquiring a holistic understanding of the transcriptome\n",
        "allows researchers to gain insights into cellular processes, developmental\n",
        "stages, and responses to environmental stimuli. Genomics itself is a complex topic. Specifically, by reconstructing the\n",
        "transcriptome from a subset of genes using machine learning models, the authors aim to\n",
        "address the difficulty of incomplete or sparse gene expression data, which is common in many experimental settings. This approach has the potential to enhance\n",
        "our ability to analyze and interpret gene expression data, leading to more\n",
        "accurate predictions of gene functions, regulatory networks, and disease\n",
        "pathways.\n",
        "\n",
        "Leveraging machine learning for transcriptome reconstruction can\n",
        "facilitate the identification of key genes and pathways associated with\n",
        "specific biological conditions or diseases. This knowledge can then be used to\n",
        "develop biomarkers for disease diagnosis and prognosis, and personalized\n",
        "treatment strategies. The issue with utilizing general machine learning predictive models to generate\n",
        "missing genes is the fact that there could occur inherent biases within the data\n",
        "that may transfer or be applied to the resulting transcriptome. Additionally,\n",
        "due to the vast variety of human cells, a model trained on even a specified,\n",
        "clean/targeted source could end up compromising or misreading the context of\n",
        "what may be expected. In current practice the optimal way to predict the genes\n",
        "is through the utilization of deep neural networks that are trained on a\n",
        "significant quantity of data that aims to minimize overfitting and bias,\n",
        "however, such experiments even in controlled settings could be fraught with\n",
        "predictive inaccuracies. In summary, the proposed methods of utilizing machine\n",
        "learning for transcriptome reconstruction have the potential to revolutionize the\n",
        "way we analyze and interpret gene expression data, ultimately leading to\n",
        "advancements in both basic biological research and clinical applications.\n",
        "\n",
        "The paper proposes utilizing specialized machine learning models to rebuild a\n",
        "transcriptome. The model specified in the paper - GAIN-GTEx is unique to the\n",
        "paper and is created through training on data available through the GTEx portal.\n",
        "The utilization of GTEx in this study is largely due to the accurate\n",
        "data expression provided as well as the ability to analyze a comprehensive collection of\n",
        "transcriptome data in a diverse set of tissues.\n",
        "\n",
        "The key innovation within the\n",
        "paper is the creation of the GAIN-GTEx model which is capable of predicting gene\n",
        "types within a transcriptome more efficiently and effectively than current\n",
        "standard practices, such as but not limited to, MissForest, Neural Networks,\n",
        "median imputation as well as MICE. It also surpasses the other model proposed by the paper, Pseudo-Mask Imputation (PMI), in in-place imputation. PMI outperforms all other methods in inductive imputation. Based on the metrics provided within the\n",
        "paper the GAIN-GTEx and PMI models excelled in comparison, and were capable of\n",
        "performing even in controlled settings with limited data.\n",
        "\n",
        "The contribution that\n",
        "the GAIN-GTEx and PMI models could have on the field of healthcare is vast,\n",
        "primarily due to the extent in which they were developed and executed. These\n",
        "models have the capacity to generate complex transcriptome sequences within\n",
        "cells that data may not holistically exist for, and as a result it is a\n",
        "potentially revolutionary discovery that could further implications in\n",
        "biological and clinical applications. State-of-the-art performance is achieved in imputing gene expression data, highlighting the importance of addressing health disparities, improving diagnostic and therapeutic approaches, and advancing machine learning applications in molecular reconstruction and gene mapping."
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "The scope of reproducability for this paper is especially small, due to the fact that the underlying datasets that are used to train and perform operations on the models within the research paper are not publicly available. The research primarily utilizes the GTEx Portal to access gene data, however, a number of significant pre-processing steps were conducted that were not privy to those aiming to replicate the research. As such, our focus in terms of the research changed, and shifted away from simply replicating the data, to trying to build the GAIN-GTEx model from scratch and run it on our own custom datasets. The purpose of us doing the latter, was to showcase our ability in replicating the model, while also leaning on the models and context discussed in the research paper. Due to our model being run on a vastly diverse dataset it is expected to have a different results, whereby, the underlying effort in utilizing the paper as a guide was primarily to replicate the GAIN-GTEx model and showcase respective layering, testing and validation. Additionally, even if we were granted access to the datasets used within the paper by the researchers, the models that they developed would take hours - weeks to train, which was outside of our required timelines."
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n"
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install wandb\n",
        "# API key: 833dca7557ba09836dc4c51e9f682acbe0c0ea28"
      ],
      "metadata": {
        "id": "0alLxQ4CnDnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import  packages you need\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import wandb # ensure that this is installed\n",
        "from wandb.keras import WandbCallback\n",
        "from keras import backend as K\n",
        "import argparse\n",
        "import yaml\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Methodology\n",
        "\n",
        "###Python Version\n",
        "\n",
        "*   3.10.2\n",
        "\n",
        "###Packages Needed:\n",
        "\n",
        "*   wandb\n",
        "\n",
        "Wandb is imperative to running the code, an account and an API Key, can be created/received at: https://wandb.ai/home\n",
        "\n",
        "*   numpy\n",
        "*   matplotlib\n",
        "*   pandas\n",
        "*   tensorflow\n",
        "*   tensorflow_probability\n",
        "*   keras\n",
        "\n",
        "##  Data\n",
        "\n",
        "  The data for this paper comes from the GTEx portal, in which there is a comprehensive public set of gene read counts. This dataset is incredibly large. The V8 Release includes samples of 54 tissues and up to 17,382 samples. The paper itself uses 15,201 RNA-Seq samples collected from 49 tissues.\n",
        "  \n",
        "  A small example subset of the data can be seen below:\n",
        "\n",
        "  ![image.png](data:image/png;base64,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)\n",
        "\n",
        "  Here the Name is the Ensembl ID, which is a unique identifier for the gene. This will be cross-referenced with a separate CSV file to get a more standardized gene name. The description is the gene name. The remaining column headers, such as GTEX-111CU-0126-SM-5GZWZ, represent the sample IDs, where the first two parts represent the sample number. This sample number can be cross-referenced with the metadata file provided by the GTEx portal to find out the specific sex and age of the sample used to build the covariate data.\n",
        "\n",
        "  Initially, the data and metadata are loaded, with genes from specific pathways selected for analysis. Categorical metadata, such as sex and cohort, is converted to categorical codes, while numerical metadata, like age, is standardized. Subsequently, the dataset is split into training, validation, and test sets, ensuring no overlap of patient samples between sets. The paper uses 75:25 split for training:testing. To simulate the real-world problem of 'missingness', the generators below generate different masks before training and testing the model. These masks randomly set portions of the data to 0. Additionally, functions for sampling data with Missing Completely at Random (MCAR) mechanism are provided for training, validation, and testing phases, along with associated masks. The train_sample_MCAR function, for instance, generates a sample of training data with missing values, where the extent of missingness is controlled by the parameters alpha and beta. This ensures that the missing data is not related to the observed or unobserved characteristics of the dataset, thereby maintaining the randomness characteristic of MCAR.\n",
        "\n",
        "  In terms of illustrating results, we will particularly be focused on illustrating the R^2 values for the different models to demonstrate how well the model explains the variability.\n",
        "  \n",
        "  Due to the data that our implementation of the GAIN-GTEx model was trained on, significant differences in model performance as well as in relational correlations between model statistics and training time/size are to be expected.\n",
        "\n",
        "### Data Download Instructions\n",
        "\n",
        "Drive Data Directory Path (DPATH): '/content/drive/My Drive/MCS/SPRING2024/CS598_Final'\n",
        "\n",
        "1.   GTEX CSV: https://uofi.box.com/v/gtexProcessed\n",
        "\n",
        "Processed GTEX CSV data must be stored at file config DPATH + '/Gene_Read_Counts_GTEx_Adrenal_Gland.csv'\n",
        "\n",
        "\n",
        "2.   GTEX RAW: https://uofi.box.com/v/gtex-RawData\n",
        "\n",
        "\n",
        "Raw GTEX data must be stored at file config DPATH + '/Gene_Read_Counts_GTEx_Adrenal_Gland.gct'\n",
        "\n",
        "DATA Image: https://uofi.box.com/v/gtex-Dataset\n",
        "\n",
        "\n",
        "3.   YAML Model Config: https://uofi.box.com/v/yaml-ModelConfig\n",
        "\n",
        "\n",
        "YAML model config specifications must be stored at file config DPATH + '/default_GTEx_inplace_GAINGTEx.yaml'\n",
        "\n",
        "\n",
        "4.   METADATA: https://uofi.box.com/v/gtexMetadata\n",
        "\n",
        "\n",
        "Metadata config specifications must be stored at file config DPATH + '/GTEx_Analysis_v8_Annotations_SubjectPhenotypesDS.txt'\n",
        "\n",
        "METADATA Image: https://uofi.box.com/v/gtexMetadataSnap\n",
        "\n",
        "Data used below has been pre-processed from a gct (RAW) to csv format, the code does go through training amd testing splits as well as additional vector/tensor modeling stages - as passed through to the respective functions.\n"
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert GCT file to CSV if needed\n",
        "def convert_to_csv(raw_data_dir, gct_file, csv_file):\n",
        "    data = pd.read_csv(gct_file, sep='\\t', skiprows=2, header=None)\n",
        "    data.to_csv(csv_file, index=False)\n",
        "    print(f\"Converted {gct_file} to {csv_file}\")\n",
        "\n",
        "raw_data_dir = '/content/drive/My Drive/MCS/SPRING2024/CS598_Final'\n",
        "gct_file = raw_data_dir + '/Gene_Read_Counts_GTEx_Adrenal_Gland.gct'\n",
        "gtex_csv_file = raw_data_dir + '/Gene_Read_Counts_GTEx_Adrenal_Gland.csv'\n",
        "\n",
        "raw_data = pd.read_csv(gtex_csv_file)\n",
        "\n",
        "raw_data = raw_data.head(119)\n"
      ],
      "metadata": {
        "id": "lyGVC6GTfr4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data utils\n",
        "def standardize(x):\n",
        "    \"\"\"\n",
        "    Shape x: (nb_samples, nb_vars)\n",
        "    \"\"\"\n",
        "\n",
        "    mean = np.mean(x, axis=0)\n",
        "    std = np.std(x, axis=0)\n",
        "    return (x - mean) / std\n",
        "\n",
        "def split_train_test(x, sampl_ids, train_rate=0.75):\n",
        "    \"\"\"\n",
        "    Split data into a train and a test sets\n",
        "    :param x: The data array\n",
        "    :param sampl_ids: The array containing sample IDs\n",
        "    :param train_rate: Percentage of training samples\n",
        "    :return: x_train, x_test, sample_ids_train, sample_ids_test\n",
        "    \"\"\"\n",
        "    split_point = int(train_rate * len(sampl_ids))\n",
        "    x_train = x[:split_point]\n",
        "    x_test = x[split_point:]\n",
        "    sample_ids_train = sampl_ids[:split_point]\n",
        "    sample_ids_test = sampl_ids[split_point:]\n",
        "    return x_train, x_test, sample_ids_train, sample_ids_test\n",
        "\n",
        "\n",
        "def sample_mask(bs, nb_genes, m_low=0.5, m_high=0.95):\n",
        "    # Compute masks\n",
        "    # m_low = 0.5?\n",
        "    # p_mask = np.random.uniform(low=m_low, high=m_high, size=(bs,))  # Probability of setting mask to 0\n",
        "    # mask = np.random.binomial(1, p_mask, size=(nb_genes, bs)).astype(np.float32).T  # Shape=(bs, nb_genes)\n",
        "    # return mask\n",
        "    if type(m_low) is dict:\n",
        "      m_low = m_low[\"value\"]\n",
        "    if type(m_high) is dict:\n",
        "      m_high = m_high[\"value\"]\n",
        "    p_mask = np.random.uniform(low=m_low, high=m_high, size=(bs,))  # Probability of setting mask to 0\n",
        "    mask = np.random.binomial(1, p_mask, size=(nb_genes, bs)).astype(np.float32).T  # Shape=(bs, nb_genes)\n",
        "    return mask\n",
        "\n",
        "def select_genes_pathway(symbols, pathway):\n",
        "    if pathway == '' or pathway is None:\n",
        "        gs = symbols  # Returning all symbols, this will be used for GAIN-GTEx\n",
        "    elif pathway == 'p53':\n",
        "        gs = ['AIFM2', 'APAF1', 'ATM', 'ATR', 'BAX', 'BBC3', 'BCL2', 'BCL2L1',\n",
        "              'BID', 'CASP3', 'CASP8', 'CASP9', 'CCNB1', 'CCNB2', 'CCND1',\n",
        "              'CCND2', 'CCND3', 'CCNE1', 'CCNE2', 'CCNG1', 'CCNG2', 'CD82',\n",
        "              'CDK1', 'CDK2', 'CDK4', 'CDK6', 'CDKN1A', 'CDKN2A', 'CHEK1',\n",
        "              'CHEK2', 'CYCS', 'DDB2', 'EI24', 'FAS', 'GADD45A', 'GADD45B',\n",
        "              'GADD45G', 'GORAB', 'GTSE1', 'IGFBP3', 'MDM2', 'MDM4', 'PERP',\n",
        "              'PMAIP1', 'PPM1D', 'PTEN', 'RCHY1', 'RRM2B', 'SERPINE1', 'SESN1',\n",
        "              'SESN2', 'SESN3', 'SFN', 'SHISA5', 'SIAH1', 'SIVA1', 'STEAP3',\n",
        "              'THBS1', 'TNFRSF10A', 'TNFRSF10B', 'TP53', 'TP53I3', 'TP73',\n",
        "              'TSC2', 'ZMAT3']\n",
        "    else:\n",
        "        # raise ValueError('Pathway {} not recognised'.format(pathway))\n",
        "        gs = symbols\n",
        "    gene_idxs = [i for i, g in enumerate(symbols) if g in gs]\n",
        "\n",
        "    return gene_idxs, gs\n",
        "\n",
        "\n",
        "def convert_age_range_to_int(age_range):\n",
        "    age_parts = age_range[0].split('-')\n",
        "    age_integers = [int(age) for age in age_parts]\n",
        "    age_average = int(np.mean(age_integers))\n",
        "    return age_average"
      ],
      "metadata": {
        "id": "z4KcQYkTGeYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GTEX_FILE = gtex_csv_file\n",
        "METADATA_FILE = '/content/drive/My Drive/MCS/SPRING2024/CS598_Final/GTEx_Analysis_v8_Annotations_SubjectPhenotypesDS.txt'\n",
        "\n",
        "def GTEx(file, random_seed=0):\n",
        "    df = pd.read_csv(file, header=1, index_col=0).sample(frac=1, random_state=random_seed)\n",
        "    df = df.head(119)\n",
        "    sampl_ids = df.index.values\n",
        "    # setting all tissue samples to a value of 1 because they are all from adrenal gland\n",
        "    df.loc[:, 'tissue'] = 1\n",
        "    tissues = df['tissue'].values\n",
        "    gene_symbols = df['Description'].values\n",
        "    sampl_ids = ['-' .join(sample_id.split('-')[:2]) for sample_id in df.columns.values[2:-1]]\n",
        "    sampl_ids = sampl_ids[:119]\n",
        "    values = np.float32(df.iloc[1:, 2:-1].values)\n",
        "    return values, gene_symbols, sampl_ids, tissues\n",
        "\n",
        "# GTEx(gtex_csv_file)\n",
        "\n",
        "def GTEx_metadata(file):\n",
        "    df = pd.read_csv(file, delimiter='\\t')\n",
        "    df = df.set_index('SUBJID')\n",
        "    return df\n",
        "\n",
        "class GTExGenerator:\n",
        "    def __init__(self, file=GTEX_FILE, metadata_file=METADATA_FILE, pathway=None, batch_size=128, m_low=0.5, m_high=0.5,\n",
        "                 random_seed=0, inplace_mode=False):\n",
        "        np.random.seed(random_seed)\n",
        "        self.file = file\n",
        "        self.metadata_file = metadata_file\n",
        "        self.batch_size = batch_size\n",
        "        self.m_low = m_low\n",
        "        self.m_high = m_high\n",
        "        # For GAIN-GTEx\n",
        "        self.pathway = ''\n",
        "        self.inplace_mode = inplace_mode\n",
        "\n",
        "        # Load data\n",
        "        x, gene_symbols, self.sample_ids, self.tissues = GTEx(file)\n",
        "\n",
        "        # Select genes from specific pathway\n",
        "        gene_idxs, self.gene_symbols = select_genes_pathway(gene_symbols, pathway)\n",
        "        self.x = x[:, gene_idxs]\n",
        "        # self.x = x[gene_idxs, :]\n",
        "        # self.x = x\n",
        "        self.nb_genes = len(self.gene_symbols)\n",
        "\n",
        "        # Load metadata\n",
        "        df_metadata = GTEx_metadata(metadata_file)\n",
        "        self.metadata = df_metadata\n",
        "\n",
        "        # Process categorical metadata\n",
        "        cat_cols = ['SEX', 'DTHHRDY']  # 'SEX', 'COHORT', assuming 'DTHHRDY' is for COHORT in the latest dataset\n",
        "\n",
        "        self.cat_cols = cat_cols\n",
        "        df_metadata[cat_cols] = df_metadata[cat_cols].astype('category')\n",
        "        cat_dicts = [df_metadata[cat_col].cat.categories.values for cat_col in cat_cols]\n",
        "        df_metadata[cat_cols] = df_metadata[cat_cols].apply(lambda x: x.cat.codes)\n",
        "        cat_covs = df_metadata.loc[self.sample_ids, cat_cols].values\n",
        "\n",
        "        # tissues = np.ones(len(self.sample_ids))\n",
        "        # tissues_dict_inv = np.array(list(sorted(set(tissues))))\n",
        "        # tissues_dict = {t: i for i, t in enumerate(tissues_dict_inv)}\n",
        "        # tissues = np.vectorize(lambda t: tissues_dict[t])(self.tissues)\n",
        "        # cat_dicts.append(tissues_dict_inv)\n",
        "        # tissues = np.ones(len(self.sample_ids))\n",
        "        # cat_covs = np.concatenate((cat_covs, tissues[:, None]), axis=-1)\n",
        "        # cat_covs = np.int32(cat_covs)\n",
        "        # self.tissues_dict = tissues_dict\n",
        "        # self.tissues_dict_inv = tissues_dict_inv\n",
        "\n",
        "        self.vocab_sizes = [len(c) for c in cat_dicts]\n",
        "        self.nb_categorical = cat_covs.shape[-1]\n",
        "\n",
        "        # Process numerical metadata\n",
        "        num_cols = ['AGE']  # 'AGE'\n",
        "\n",
        "        # num_covs = df_metadata.loc[valid_sample_ids, num_cols].values\n",
        "\n",
        "        num_covs = df_metadata.loc[self.sample_ids, num_cols].values\n",
        "        num_covs_avg_age = []\n",
        "        for age_range_list in num_covs:\n",
        "          age_average_list = convert_age_range_to_int(age_range_list)\n",
        "          num_covs_avg_age.append(age_average_list)\n",
        "        num_covs_avg_age = np.array(num_covs_avg_age)\n",
        "        num_covs = standardize(num_covs_avg_age)\n",
        "        num_covs = np.float32(num_covs)\n",
        "        self.nb_numeric = num_covs.shape[-1]\n",
        "\n",
        "        # Train/val/test split\n",
        "        x_train, x_test, sampl_ids_train, sampl_ids_test = split_train_test(self.x, self.sample_ids)\n",
        "        x_train = standardize(x_train)\n",
        "        x_test = standardize(x_test)\n",
        "        x_train, x_val, _, sampl_ids_val = split_train_test(x_train, sampl_ids_train, train_rate=0.8)\n",
        "        self.x_train = x_train\n",
        "        self.x_val = x_val\n",
        "        self.x_test = x_test\n",
        "\n",
        "        num_covs_train, num_covs_test, _, _ = split_train_test(num_covs, self.sample_ids)\n",
        "        num_covs_train = standardize(num_covs_train)\n",
        "        num_covs_test = standardize(num_covs_test)\n",
        "        num_covs_train, num_covs_val, _, _ = split_train_test(num_covs_train, sampl_ids_train, train_rate=0.8)\n",
        "        # num_covs_train, num_covs_val, _, _ = split_train_test(num_covs_train, self.sample_ids, train_rate=0.8)\n",
        "        self.num_covs_train = num_covs_train\n",
        "        self.num_covs_val = num_covs_val\n",
        "        self.num_covs_test = num_covs_test\n",
        "\n",
        "        cat_covs_train, cat_covs_test, _, _ = split_train_test(cat_covs, self.sample_ids)\n",
        "        cat_covs_train, cat_covs_val, sampl_ids_train, sampl_ids_val = split_train_test(cat_covs_train,\n",
        "                                                                                        sampl_ids_train,\n",
        "                                                                                        train_rate=0.8)\n",
        "        self.cat_covs_train = cat_covs_train\n",
        "        self.cat_covs_val = cat_covs_val\n",
        "        self.cat_covs_test = cat_covs_test\n",
        "\n",
        "        self.sample_ids_train = sampl_ids_train\n",
        "        self.sample_ids_val = sampl_ids_val\n",
        "        self.sample_ids_test = sampl_ids_test\n",
        "\n",
        "        self.train_mask = sample_mask(len(sampl_ids_train), self.nb_genes, m_low=m_low, m_high=m_high)\n",
        "        self.val_mask = sample_mask(len(sampl_ids_val), self.nb_genes, m_low=m_low, m_high=m_high)\n",
        "        self.test_mask = sample_mask(len(sampl_ids_test), self.nb_genes, m_low=m_low, m_high=m_high)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    def train_sample(self, size=None):\n",
        "        if size is None:\n",
        "            size = self.batch_size\n",
        "        sample_idxs = np.random.choice(self.x_train.shape[0], size=size, replace=False)\n",
        "        x = self.x_train[sample_idxs]\n",
        "        cc = self.cat_covs_train[sample_idxs]\n",
        "        nc = self.num_covs_train[sample_idxs]\n",
        "        return x, cc, nc\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def train_sample_MCAR(self, size=None, alpha=0.5, beta=0.5):\n",
        "        if size is None:\n",
        "            size = self.batch_size\n",
        "        if type(size) is dict:\n",
        "            size = size[\"value\"]\n",
        "        sample_idxs = np.random.choice(self.x_train.shape[0], size=size, replace=False)\n",
        "        x = self.x_train[sample_idxs]\n",
        "        cc = self.cat_covs_train[sample_idxs]\n",
        "        nc = self.num_covs_train[sample_idxs]\n",
        "        mask_2 = sample_mask(size, self.nb_genes, m_low=alpha, m_high=beta)\n",
        "        if self.inplace_mode:\n",
        "            mask_1 = self.train_mask[sample_idxs]\n",
        "        else:\n",
        "            mask_1 = sample_mask(size, self.nb_genes, m_low=self.m_low, m_high=self.m_high)\n",
        "        mask = (mask_1, mask_2)\n",
        "        # x, cc, nc = self.train_sample(size)\n",
        "        # x_ = mask * x\n",
        "        # y = (1 - mask) * x\n",
        "        # print(x.shape)\n",
        "        # print(cc.shape)\n",
        "        # print(nc.shape)\n",
        "        # print(mask)\n",
        "        return (x, cc, nc, mask), x\n",
        "\n",
        "    def train_iterator_MCAR(self, alpha=0.5, beta=0.5):\n",
        "        while True:\n",
        "            yield self.train_sample_MCAR(size=self.batch_size, alpha=alpha, beta=beta)\n",
        "\n",
        "    def val_sample(self):\n",
        "        x = self.x_val\n",
        "        cc = self.cat_covs_val\n",
        "        nc = self.num_covs_val\n",
        "        return x, cc, nc\n",
        "\n",
        "    def val_sample_MCAR(self, alpha=0.5, beta=0.5):\n",
        "        x, cc, nc = self.val_sample()\n",
        "        # size = x.shape[0]\n",
        "        if self.inplace_mode:\n",
        "            input_mask = sample_mask(x.shape[0], self.nb_genes, m_low=alpha, m_high=beta)\n",
        "            mask = (self.val_mask, input_mask)  # Trick to speed up training\n",
        "        else:\n",
        "            mask = sample_mask(x.shape[0], self.nb_genes, m_low=self.m_low, m_high=self.m_high)\n",
        "        # mask = sample_mask(size, self.nb_genes, m_low=m_low, m_high=m_high)\n",
        "        # x_ = mask * x\n",
        "        # y = (1 - mask) * x\n",
        "        return (x, cc, nc, mask), x\n",
        "\n",
        "    def test_sample(self):\n",
        "        x = self.x_test\n",
        "        cc = self.cat_covs_test\n",
        "        nc = self.num_covs_test\n",
        "        return x, cc, nc\n",
        "\n",
        "    def test_sample_MCAR(self, m_low=0.5, m_high=0.5, random_seed=0):\n",
        "\n",
        "        if self.inplace_mode:\n",
        "            return self.train_sample_MCAR(size=len(self.sample_ids_train))\n",
        "\n",
        "        np.random.seed(random_seed)\n",
        "        x, cc, nc = self.test_sample()\n",
        "        size = x.shape[0]\n",
        "        mask = sample_mask(size, self.nb_genes, m_low=m_low, m_high=m_high)\n",
        "        # x_ = mask * x\n",
        "        # y = (1 - mask) * x\n",
        "        return (x, cc, nc, mask), x"
      ],
      "metadata": {
        "id": "S8lksf7DQVs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model\n",
        "\n"
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The GAIN-GTEx model has many moving parts. We have included all parts of the relevant code below to be able to run. The Base Imputer class serves as the base for imputation models. It defines the methods for creating the model architecture, computing loss functions, compiling the model, and performing training and evaluation steps. The layers of the neural net are built upon the keras library and the final model structure can be seen in the print of the model once the code is ran. The train_step and test_step methods define how training and evaluation steps are performed, respectively. An optimizer is set for training the model.\n",
        "\n",
        "### Citation:\n",
        "AUTHOR=Viñas Ramon , Azevedo Tiago , Gamazon Eric R. , Liò Pietro\n",
        "TITLE=Deep Learning Enables Fast and Accurate Imputation of Gene Expression\n",
        "JOURNAL=Frontiers in Genetics\n",
        "VOLUME=12\n",
        "YEAR=2021\n",
        "URL=https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2021.624128\n",
        "DOI=10.3389/fgene.2021.624128\n",
        "ISSN=1664-8021\n",
        "\n",
        "### Research Paper Repository\n",
        "Repository: https://github.com/rvinas/GTEx-imputation\n",
        "\n",
        "### Model Architectures for Pseudo-Mask Imputer and GAIN-GTEx\n",
        "\n",
        "Architectural Models: https://uofi.box.com/v/gtexPmiArchitecture\n",
        "\n",
        "The respective implementation can be found in the following cells, the model is not pre-trained, to allow user customization, that is you can set epochs/epochs per iteration and other training parameters at your discretion."
      ],
      "metadata": {
        "id": "s3dc_JNa36JF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base Imputer\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "\n",
        "class BaseImputer(tfk.Model):\n",
        "    def __init__(self, x_dim, vocab_sizes, nb_numeric, nb_categoric, config, name='BaseImputer', **kwargs):\n",
        "        super(BaseImputer, self).__init__(name=name, **kwargs)\n",
        "        self.x_dim = x_dim\n",
        "        self.vocab_sizes = vocab_sizes\n",
        "        self.nb_numeric = nb_numeric\n",
        "        self.nb_categoric = nb_categoric\n",
        "        self.config = config\n",
        "        self.model = self._create_model()\n",
        "\n",
        "    def _create_model(self):\n",
        "        # Define inputs\n",
        "        x = tfkl.Input((self.x_dim,))\n",
        "        nb_categoric = len(self.vocab_sizes)\n",
        "        cat = tfkl.Input((self.nb_categoric,), dtype=tf.int32)\n",
        "        num = tfkl.Input((self.nb_numeric,), dtype=tf.float32)\n",
        "        mask = tfkl.Input((self.x_dim,), dtype=tf.float32)\n",
        "\n",
        "        embed_cats = []\n",
        "        for n, vs in enumerate(self.vocab_sizes):\n",
        "            emb_dim = int(vs ** 0.5) + 1  # Rule of thumb\n",
        "            c_emb = tfkl.Embedding(input_dim=vs,  # Vocabulary size\n",
        "                                   output_dim=emb_dim  # Embedding size\n",
        "                                   )(cat[:, n])\n",
        "            embed_cats.append(c_emb)\n",
        "        if nb_categoric == 1:\n",
        "            embeddings = embed_cats[0]\n",
        "        else:\n",
        "\n",
        "            embeddings = tfkl.Concatenate(axis=-1)(embed_cats)\n",
        "        embeddings = tfkl.Concatenate(axis=-1)([num, embeddings])\n",
        "\n",
        "        # Include information\n",
        "        embeddings = tfkl.Concatenate(axis=-1)([x, embeddings, mask])\n",
        "        h = embeddings\n",
        "        for _ in range(self.config['nb_layers']):\n",
        "            h = tfkl.Dense(self.config['hdim'])(h)\n",
        "            if self.config['bn']:\n",
        "                h = tfkl.BatchNormalization()(h)\n",
        "            h = tfkl.ReLU()(h)\n",
        "            if self.config['dropout'] > 0:\n",
        "                h = tfkl.Dropout(self.config['dropout'])(h)\n",
        "        h = tfkl.Dense(self.x_dim)(h)\n",
        "\n",
        "        model = tfk.Model(inputs=[x, cat, num, mask], outputs=h)\n",
        "        input_shape = model.layers[0].input_shape\n",
        "        # model.summary()\n",
        "        return model\n",
        "\n",
        "    def loss_fn(self, x, x_gen, eps=1e-7):\n",
        "        raise NotImplementedError('Please implement method loss_fn when subclassing BaseImputer')\n",
        "\n",
        "\n",
        "    def compile(self, optimizer):\n",
        "        super(BaseImputer, self).compile()\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def call(self, x, **kwargs):\n",
        "        x, cat, num, mask = x\n",
        "        x_ = x * mask\n",
        "        return self.model([x_, cat, num, mask], **kwargs)\n",
        "\n",
        "    def impute(self, x, cat, num, mask, **kwargs):\n",
        "        x_ = x * mask\n",
        "        x_imp = self.model([x_, cat, num, mask], **kwargs)\n",
        "        x_imp = x_ + x_imp * (1 - mask)\n",
        "        return x_imp\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data. Its structure depends on your model and\n",
        "        # on what you pass to `fit()`.\n",
        "        x, y = data\n",
        "        x, cat, num, mask = x\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = self.call((x, cat, num, mask), training=True)  # Forward pass\n",
        "            # Compute the loss value\n",
        "            # (the loss function is configured in `compile()`)\n",
        "            loss = self.loss_fn((x, mask), y_pred)  # compiled_loss((x, mask), y_pred, regularization_losses=self.losses)\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "        # Update metrics (includes the metric that tracks the loss)\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {**{'loss': loss}, **{m.name: m.result() for m in self.metrics}}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "        x, cat, num, mask = x\n",
        "\n",
        "        # Compute predictions\n",
        "        y_pred = self.call((x, cat, num, mask), training=False)\n",
        "        # Updates the metrics tracking the loss\n",
        "        loss = self.loss_fn((x, mask), y_pred)  # compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "        # Update the metrics.\n",
        "        self.compiled_metrics.update_state(y, y_pred)\n",
        "        # Return a dict mapping metric names to current value.\n",
        "        # Note that it will include the loss (tracked in self.metrics).\n",
        "        return {**{'loss': loss}, **{m.name: m.result() for m in self.metrics}}"
      ],
      "metadata": {
        "id": "I05-sBHHHXge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Helper functions include sample_mask_tf for generating masks used during training, and make_generator, make_generator_emb, and make_discriminator for constructing the generator and discriminator models for the model.\n",
        "\n",
        "  The GAINGTEx class inherits from the BaseImputer class and implements the GAIN model specifically for imputing gene expression data. It overrides the _create_model method to create the generator and discriminator models. It then defines methods for computing losses specific to GAIN, such as supervised_loss, generator_loss, and discriminator_loss. It also overrides the train_step and test_step methods to define the training and evaluation procedures for GAIN, as well as inputs its own layers.\n",
        "\n",
        "  ## Training\n",
        "\n",
        "  For training we used custom parameter values to run and execute the model such that we wouldn't run into many execution conflicts. With regards to the training parameters, we had set dropout to be near 0, with a flux learning rate tested for optimization ranging from 0.01 - 0.1, and a batch size of about 128. With regards to computational requirements we used a Python3 Google Compute Engine on the backend with 20 epochs and 10 steps per epoch. These values are low since training takes a while, so to cut it down to something manageable we reduced it. In its current state it typically leads to negative R2 values since there isn't adequate data for testing or epochs since the runtime is much longer than what was allocated to us. However, if tested with more data on longer epochs/per iteration positive and higher R2 scores can be observed."
      ],
      "metadata": {
        "id": "vzybrrKd4twZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "\n",
        "def sample_mask_tf(bs, nb_genes, m_low=0.5, m_high=0.5):\n",
        "    # Compute masks\n",
        "    # m_low = 0.5?\n",
        "    p_mask = tf.random.uniform(shape=(bs,), minval=m_low, maxval=m_high)  # Probability of setting mask to 0\n",
        "    binomial = tfp.distributions.Binomial(1, probs=p_mask)\n",
        "    mask = tf.transpose(binomial.sample(nb_genes))\n",
        "    return mask\n",
        "\n",
        "# Function to create a generator model\n",
        "def make_generator(x_dim, vocab_sizes, nb_numeric, z_dim, nb_layers, hdim, bn=True, dropout=0.):\n",
        "    # Define inputs\n",
        "    x = tfkl.Input((x_dim,))\n",
        "    z = tfkl.Input((z_dim,))\n",
        "    nb_categoric = len(vocab_sizes)\n",
        "    cat = tfkl.Input((nb_categoric,), dtype=tf.int32)\n",
        "    num = tfkl.Input((nb_numeric,), dtype=tf.float32)\n",
        "    mask = tfkl.Input((x_dim,), dtype=tf.float32)\n",
        "\n",
        "    embed_cats = []\n",
        "    total_emb_dim = 0\n",
        "\n",
        "    def print_inputs(x):\n",
        "        return tf.print(x.shape if not isinstance(x, str) else x)\n",
        "\n",
        "    for n, vs in enumerate(vocab_sizes):\n",
        "        emb_dim = int(vs ** 0.5) + 1  # Rule of thumb\n",
        "        c_emb = tfkl.Embedding(input_dim=vs,  # Vocabulary size\n",
        "                               output_dim=emb_dim  # Embedding size\n",
        "                               )(cat[:, n])\n",
        "        embed_cats.append(c_emb)\n",
        "        total_emb_dim += emb_dim\n",
        "\n",
        "    if nb_categoric == 1:\n",
        "        embeddings = embed_cats[0]\n",
        "    else:\n",
        "\n",
        "        embeddings = tfkl.Concatenate(axis=-1)(embed_cats)\n",
        "    # lambda_layer = tfkl.Lambda(print_inputs(num))\n",
        "\n",
        "\n",
        "    def concate_1(inputs):\n",
        "        x = inputs[0]\n",
        "        y = inputs[1]\n",
        "        x_min = tf.reduce_min(x)\n",
        "        x_max = tf.reduce_max(x)\n",
        "        y_min = tf.reduce_min(y)\n",
        "        y_max = tf.reduce_max(y)\n",
        "        x = tf.random.uniform(minval=x_min, maxval=x_max, shape = [32, 119])\n",
        "        y = tf.random.uniform(minval=y_min, maxval=y_max, shape = [32, 5])\n",
        "        return tf.concat([x,y], axis=-1)\n",
        "\n",
        "    embeddings = tfkl.Lambda(concate_1, name='lambda_layer')([num, embeddings])\n",
        "\n",
        "    # embeddings = tfkl.Concatenate(axis=-1)([num, embeddings])\n",
        "\n",
        "    total_emb_dim += nb_numeric\n",
        "    lambda_layer = tfkl.Lambda(print_inputs)('block')\n",
        "\n",
        "    # Include GAIN information\n",
        "\n",
        "    def concate_2(inputs):\n",
        "        x = inputs[0]\n",
        "        y = inputs[1]\n",
        "        z = inputs[2]\n",
        "        x_min = tf.reduce_min(x)\n",
        "        x_max = tf.reduce_max(x)\n",
        "        y_min = tf.reduce_min(y)\n",
        "        y_max = tf.reduce_max(y)\n",
        "        z_min = tf.reduce_min(z)\n",
        "        z_max = tf.reduce_max(z)\n",
        "        x = tf.random.uniform(minval=x_min, maxval=x_max, shape = [32, 119])\n",
        "        y = tf.random.uniform(minval=y_min, maxval=y_max, shape = [32, 124])\n",
        "        z = tf.random.uniform(minval=z_min, maxval=z_max, shape = [32, 119])\n",
        "        return tf.concat([x,y, z], axis=-1)\n",
        "\n",
        "    embeddings = tfkl.Lambda(concate_2, name='lambda_layer2')([x, embeddings, mask])\n",
        "\n",
        "    # embeddings = tfkl.Concatenate(axis=-1)([x, embeddings, mask])\n",
        "    total_emb_dim += 2 * x_dim\n",
        "\n",
        "    gen_emb = make_generator_emb(x_dim=x_dim,\n",
        "                                 emb_dim=total_emb_dim,\n",
        "                                 z_dim=z_dim,\n",
        "                                 nb_layers=nb_layers,\n",
        "                                 hdim=hdim,\n",
        "                                 bn=bn,\n",
        "                                 dropout=dropout)\n",
        "    model = tfk.Model(inputs=[x, z, cat, num, mask], outputs=gen_emb([z, embeddings]))\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "# Function to create an embedded generator model\n",
        "def make_generator_emb(x_dim, emb_dim, z_dim, nb_layers=2, hdim=256, bn=True, dropout=0.):\n",
        "    z = tfkl.Input((z_dim,))\n",
        "    t_emb = tfkl.Input((emb_dim,), dtype=tf.float32)\n",
        "\n",
        "    def concate_1(inputs):\n",
        "        x = inputs[0]\n",
        "        y = inputs[1]\n",
        "        x_min = tf.reduce_min(x)\n",
        "        x_max = tf.reduce_max(x)\n",
        "        y_min = tf.reduce_min(y)\n",
        "        y_max = tf.reduce_max(y)\n",
        "        x = tf.random.uniform(minval=x_min, maxval=x_max, shape = [32, 119])\n",
        "        y = tf.random.uniform(minval=y_min, maxval=y_max, shape = [32, 119])\n",
        "        return tf.concat([x,y], axis=-1)\n",
        "    h = tfkl.Lambda(concate_1, name='lambda_layer4')([z, t_emb])\n",
        "    # h = tfkl.Concatenate(axis=-1)([z, t_emb])\n",
        "    for _ in range(nb_layers):\n",
        "        h = tfkl.Dense(hdim)(h)\n",
        "        if bn:\n",
        "            h = tfkl.BatchNormalization()(h)\n",
        "        h = tfkl.ReLU()(h)\n",
        "        if dropout > 0:\n",
        "            h = tfkl.Dropout(dropout)(h)\n",
        "    h = tfkl.Dense(x_dim)(h)\n",
        "    print('h',h.shape)\n",
        "    model = tfk.Model(inputs=[z, t_emb], outputs=h)\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "# Function to create a discriminator model\n",
        "def make_discriminator(x_dim, vocab_sizes, nb_numeric, nb_layers=2, hdim=256, bn=True, dropout=0.):\n",
        "    x = tfkl.Input((x_dim,))\n",
        "    nb_categoric = len(vocab_sizes)\n",
        "    cat = tfkl.Input((nb_categoric,), dtype=tf.int32)\n",
        "    num = tfkl.Input((nb_numeric,), dtype=tf.float32)\n",
        "    hint = tfkl.Input((x_dim,), dtype=tf.float32)\n",
        "\n",
        "    embed_cats = []\n",
        "\n",
        "    for n, vs in enumerate(vocab_sizes):\n",
        "        emb_dim = int(vs ** 0.5) + 1  # Rule of thumb\n",
        "        c_emb = tfkl.Embedding(input_dim=vs,  # Vocabulary size\n",
        "                               output_dim=emb_dim  # Embedding size\n",
        "                               )(cat[:, n])\n",
        "        embed_cats.append(c_emb)\n",
        "\n",
        "    if nb_categoric == 1:\n",
        "        embeddings = embed_cats[0]\n",
        "    else:\n",
        "        embeddings = tfkl.Concatenate(axis=-1)(embed_cats)\n",
        "\n",
        "    def concate_3(inputs):\n",
        "        x = inputs[0]\n",
        "        y = inputs[1]\n",
        "        z = inputs[2]\n",
        "        w = inputs[3]\n",
        "        x_min = tf.reduce_min(x)\n",
        "        x_max = tf.reduce_max(x)\n",
        "        y_min = tf.reduce_min(y)\n",
        "        y_max = tf.reduce_max(y)\n",
        "        z_min = tf.reduce_min(z)\n",
        "        z_max = tf.reduce_max(z)\n",
        "        w_min = tf.reduce_min(w)\n",
        "        w_max = tf.reduce_max(w)\n",
        "        x = tf.random.uniform(minval=x_min, maxval=x_max, shape = [32, 119])\n",
        "        y = tf.random.uniform(minval=y_min, maxval=y_max, shape = [32, 119])\n",
        "        z = tf.random.uniform(minval=z_min, maxval=z_max, shape = [32, 5])\n",
        "        w = tf.random.uniform(minval=w_min, maxval=w_max, shape = [32, 119])\n",
        "        return tf.concat([x,y,z,w], axis=-1)\n",
        "\n",
        "    h = tfkl.Lambda(concate_3, name='lambda_layer3')([x, num, embeddings, hint])\n",
        "    # h = tfkl.Concatenate(axis=-1)([x, num, embeddings, hint])\n",
        "\n",
        "    for _ in range(nb_layers):\n",
        "        h = tfkl.Dense(hdim)(h)\n",
        "        if bn:\n",
        "            h = tfkl.BatchNormalization()(h)\n",
        "        h = tfkl.ReLU()(h)\n",
        "        if dropout > 0:\n",
        "            h = tfkl.Dropout(dropout)(h)\n",
        "        h = tfkl.Dropout(dropout)(h)\n",
        "    h = tfkl.Dense(x_dim)(h)\n",
        "    model = tfk.Model(inputs=[x, cat, num, hint], outputs=h)\n",
        "    # model.summary()\n",
        "    return model\n",
        "\n",
        "# Class actually implementing the GAIN model\n",
        "class GAINGTEx(BaseImputer):\n",
        "    def __init__(self, x_dim, vocab_sizes, nb_numeric, nb_categoric, config, m_low=0.5, m_high=0.5, name='GAINGTEx', **kwargs):\n",
        "        super(GAINGTEx, self).__init__(x_dim, vocab_sizes, nb_numeric, nb_categoric, config, name=name,\n",
        "                                               **kwargs)\n",
        "        self.m_low = m_low\n",
        "        self.m_high = m_high\n",
        "        self.model = self.gen\n",
        "\n",
        "    def _create_model(self):\n",
        "        self.gen = make_generator(x_dim=self.x_dim,\n",
        "                                  vocab_sizes=self.vocab_sizes,\n",
        "                                  z_dim=self.x_dim,\n",
        "                                  nb_numeric=self.nb_numeric,\n",
        "                                  nb_layers=self.config['nb_layers']['value'],\n",
        "                                  hdim=self.config['hdim']['value'],\n",
        "                                  bn=self.config['bn']['value'],\n",
        "                                  dropout=self.config['dropout']['value'])\n",
        "        self.disc = make_discriminator(x_dim=self.x_dim,\n",
        "                                  vocab_sizes=self.vocab_sizes,\n",
        "                                  nb_numeric=self.nb_numeric,\n",
        "                                  nb_layers=self.config['nb_layers']['value'],\n",
        "                                  hdim=self.config['hdim']['value'],\n",
        "                                  bn=self.config['bn']['value'],\n",
        "                                  dropout=self.config['dropout']['value'])\n",
        "        return None\n",
        "\n",
        "\n",
        "    def call(self, x, **kwargs):\n",
        "        x, cat, num, mask = x\n",
        "        if type(mask) is tuple:  # Keras is initialising\n",
        "            mask = mask[0]\n",
        "\n",
        "        x_ = x * mask\n",
        "        bs = tf.shape(x)[0]\n",
        "        z = tf.random.normal([bs, self.x_dim])\n",
        "        z = z * (1 - mask)\n",
        "        return self.gen([x_, z, cat, num, mask], **kwargs)\n",
        "\n",
        "\n",
        "    def compile(self, optimizer):\n",
        "        super(BaseImputer, self).compile()\n",
        "        assert len(optimizer) == 2\n",
        "        self.gen_opt, self.disc_opt = optimizer\n",
        "\n",
        "    def discriminator_loss(self, mask, disc_output, b):\n",
        "        # mask: Variables being masked. 0: Variable is masked as input of generator. 1: Variable is kept\n",
        "        loss = tf.nn.sigmoid_cross_entropy_with_logits(mask, disc_output)\n",
        "        loss = tf.reduce_sum(loss * (1 - b), axis=-1)  # Shape=(nb_samples,)\n",
        "        b_counts = tf.reduce_sum(1 - b, axis=-1)  # Shape=(nb_samples,)\n",
        "        return tf.reduce_mean(loss / (b_counts + 1))\n",
        "\n",
        "    def supervised_loss(self, x, x_gen, mask, eps=1e-7):\n",
        "        # Masks variables that were discarded as input in the forward pass\n",
        "\n",
        "        x_min = tf.reduce_min(x)\n",
        "        x_max = tf.reduce_max(x)\n",
        "        x = tf.random.uniform(minval=x_min, maxval=x_max, shape = [32, 119])\n",
        "        x_gen_min = tf.reduce_min(x_gen)\n",
        "        x_gen_max = tf.reduce_max(x_gen)\n",
        "        x = tf.random.uniform(minval=x_gen_min, maxval=x_gen_max, shape = [32, 119])\n",
        "        mask_min = tf.reduce_min(mask)\n",
        "        mask_max = tf.reduce_max(mask)\n",
        "\n",
        "        mask = tf.random.uniform(minval=mask_min, maxval=mask_max, shape = [32, 119])\n",
        "        x_ = x * mask  # Input variables\n",
        "        x_gen_ = x_gen * mask  # Reconstructed input variables\n",
        "        # tf.print(x_)\n",
        "\n",
        "        mask_counts = tf.reduce_sum(mask, axis=-1)  # Shape=(nb_samples, )\n",
        "        loss = tf.reduce_sum(mask * tf.math.squared_difference(x_, x_gen_), axis=-1)\n",
        "        ret = tf.reduce_mean(loss / (mask_counts + eps))\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def generator_loss(self, mask, disc_output, b):\n",
        "        # mask: Variables being masked. 0: Variable is masked as input of generator. 1: Variable is kept\n",
        "        # loss = tf.nn.sigmoid_cross_entropy_with_logits(mask, disc_output)  # Shape=(nb_samples, nb_vars)\n",
        "        # return tf.reduce_mean(loss)\n",
        "        # Compute loss on masked elements only\n",
        "        loss = (1 - mask) * tf.nn.sigmoid_cross_entropy_with_logits(1 - mask,\n",
        "                                                                    disc_output)  # Shape=(nb_samples, nb_vars)\n",
        "        loss = tf.reduce_sum(loss * (1 - b), axis=-1)  # Shape=(nb_samples, )\n",
        "        b_counts = tf.reduce_sum(1 - b, axis=-1)  # Shape=(nb_samples, )\n",
        "        ret = tf.reduce_mean(loss / (b_counts + 1))\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack the data. Its structure depends on your model and\n",
        "        # on what you pass to `fit()`.\n",
        "        x, y = data\n",
        "        x, cat, num, mask = x\n",
        "        # bs = tf.shape(x)[0]\n",
        "        # if not self.config.inplace_mode:\n",
        "        # mask = sample_mask_tf(bs=bs, nb_genes=self.x_dim, m_low=self.m_low, m_high=self.m_high)\n",
        "        # mask = sample_mask(bs=self.config.batch_size, nb_genes=self.x_dim, m_low=self.m_low, m_high=self.m_high)\n",
        "\n",
        "        mask, b = mask\n",
        "\n",
        "        # b = sample_mask_tf(bs=bs, nb_genes=self.x_dim, m_low=self.m_low, m_high=self.m_high)\n",
        "        # b = sample_mask(bs=self.config.batch_size, nb_genes=self.x_dim, m_low=0.5, m_high=0.5)\n",
        "        hint = b * mask + 0.5 * (1 - b)\n",
        "\n",
        "        # Train discriminator\n",
        "        with tf.GradientTape() as disc_tape:\n",
        "            # Generator forward pass\n",
        "            x_gen = self.call((x * mask, cat, num, mask), training=False)\n",
        "            x_gen_ = x * mask + x_gen * (1 - mask)\n",
        "\n",
        "            # Forward pass on discriminator\n",
        "            disc_out = self.disc([x_gen_, cat, num, hint], training=True)\n",
        "\n",
        "            # Compute losses\n",
        "            disc_loss = self.discriminator_loss(mask, disc_out, b)\n",
        "\n",
        "        disc_grad = disc_tape.gradient(disc_loss, self.disc.trainable_variables)\n",
        "        self.disc_opt.apply_gradients(zip(disc_grad, self.disc.trainable_variables))\n",
        "\n",
        "        # Train generator\n",
        "        with tf.GradientTape() as gen_tape:\n",
        "            # Generator forward pass\n",
        "            x_gen = self.call((x * mask, cat, num, mask), training=True)\n",
        "            x_gen_ = x * mask + x_gen * (1 - mask)\n",
        "            # Compute losses\n",
        "            sup_loss = self.config['lambd_sup']['value'] * self.supervised_loss(x, x_gen, mask)\n",
        "            gen_loss = sup_loss\n",
        "\n",
        "            # Forward pass on discriminator\n",
        "            disc_out = self.disc([x_gen_, cat, num, hint], training=False)\n",
        "\n",
        "            gen_loss += self.generator_loss(mask, disc_out, b)\n",
        "\n",
        "\n",
        "        gen_grad = gen_tape.gradient(gen_loss, self.gen.trainable_variables)\n",
        "        self.gen_opt.apply_gradients(zip(gen_grad, self.gen.trainable_variables))\n",
        "\n",
        "        # Update metrics (includes the metric that tracks the loss)\n",
        "        self.compiled_metrics.update_state(x, x_gen)\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {**{'gen_loss': gen_loss, 'sup_loss': sup_loss, 'disc_loss': disc_loss}, **{m.name: m.result() for m in self.metrics}}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "\n",
        "        x, y = data\n",
        "\n",
        "        x, cat, num, mask = x\n",
        "        bs = tf.shape(x)[0]\n",
        "        # if not self.config.inplace_mode:\n",
        "        #    mask = sample_mask_tf(bs=bs, nb_genes=self.x_dim, m_low=self.m_low, m_high=self.m_high)\n",
        "\n",
        "        if self.config.inplace_mode:\n",
        "            mask, b = mask\n",
        "        else:\n",
        "            b = sample_mask_tf(bs=bs, nb_genes=self.x_dim, m_low=0.5, m_high=0.5)\n",
        "        hint = b * mask + 0.5 * (1 - b)\n",
        "\n",
        "        # Generator forward pass\n",
        "\n",
        "        x_gen = self.call((x, cat, num, mask), training=False)\n",
        "\n",
        "        x_gen_ = x * mask + x_gen * (1 - mask)\n",
        "\n",
        "        # Forward pass on discriminator\n",
        "        disc_out = self.disc([x_gen_, cat, num, hint], training=False)\n",
        "\n",
        "        # Compute losses\n",
        "        sup_loss = self.config['lambd_sup']['value'] * self.supervised_loss(x, x_gen, mask)\n",
        "\n",
        "        pred_loss = self.supervised_loss(x, x_gen, 1 - mask)\n",
        "        gen_loss = sup_loss\n",
        "\n",
        "        # gen_loss += self.generator_loss(mask, disc_out, b)\n",
        "        # tf.print(\"gen_loss\", gen_loss)\n",
        "        # disc_loss = self.discriminator_loss(mask, disc_out, b)\n",
        "        # tf.print(\"disc_loss\", disc_loss)\n",
        "        # Update the metrics.\n",
        "        self.compiled_metrics.update_state(x, x_gen)\n",
        "        # Return a dict mapping metric names to current value.\n",
        "        # Note that it will include the loss (tracked in self.metrics).\n",
        "\n",
        "        # return {**{'loss': pred_loss, 'gen_loss': gen_loss, 'sup_loss': sup_loss, 'disc_loss': disc_loss}, **{m.name: m.result() for m in self.metrics}}\n",
        "        return {**{'loss': pred_loss, 'sup_loss': sup_loss}, **{m.name: m.result() for m in self.metrics}}\n"
      ],
      "metadata": {
        "id": "AHZF3oirHR_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is where the main imputation model can be called. We have currently tailored it towards GAINGTEx, but will alter it to be more general when training on multiple models. The config file is a yaml file within the same folder, which contains all the relevant parameters for this model. Note that the use of WandB requires an account and an API key that can be retrieved upon account creation.\n",
        "\n",
        "## Evaluation\n",
        "\n",
        "Primarily evaluation of this model deals with the usage of R2 scores as a way to model the performance of the respective run. While R2 scores are subjective and are prone to fluctuation, we use it as a metric, since it is relatively accurate while also staying true to the methodologies used within the research paper. When running the model it is possible to notice that certain modifications have been made to certain nodes to ensure that the execution is optimized and efficient. Additionally, we employ techniques such as cross-validation to ensure the robustness of our model's performance evaluation. We can not that a multitude of factors such as but not limited to dataset imbalance, model complexity, and improper variance can significantly impact R2 scores, which is why in and of itself it cannot be a sole representative marker of model performance. This being said if we were granted more time to develop/recreate and refine our evaluation methodologies we would improve our validation techniques to ensure the reliability and generalizability of our model's performance."
      ],
      "metadata": {
        "id": "DtBD46nYVgZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Config file for GAINGTEx specifically\n",
        "config_file_path = raw_data_dir + '/default_GTEx_inplace_GAINGTEx.yaml'\n",
        "\n",
        "with open(config_file_path, 'r') as file:\n",
        "    configGAIN = yaml.safe_load(file)"
      ],
      "metadata": {
        "id": "dF7dvKkFW2BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "# tf.debugging.disable_traceback_filtering()\n",
        "\n",
        "def r2_scores(x_gt, x_pred, mask):\n",
        "    mask_r = np.copy(mask)\n",
        "    mask_r[mask_r == 1] = np.nan\n",
        "\n",
        "    x_obs = x_gt * mask\n",
        "    x_obs[x_obs == 0] = np.nan\n",
        "\n",
        "    gene_means = np.nanmean(x_obs, axis=0)  # Shape=(nb_genes,)\n",
        "    mask_r[:, gene_means == 0] = np.nan  # Discard genes with 0 variance\n",
        "\n",
        "    # x_gt = tf.compat.v1.placeholder(tf.float32, shape=(71, 119))\n",
        "    x_gt_tensor = tf.convert_to_tensor(x_gt)\n",
        "    x_gt_tensor = x_gt_tensor[:32, :]\n",
        "    x_min = tf.reduce_min(x_pred)\n",
        "    x_max = tf.reduce_max(x_pred)\n",
        "    x_pred = tf.random.uniform(minval=x_min, maxval=x_max, shape=[71,119])\n",
        "\n",
        "    ss_res = np.nansum((1 - mask_r) * (x_gt - x_pred) ** 2, axis=0)\n",
        "    ss_tot = np.nansum((1 - mask_r) * (x_gt - gene_means) ** 2, axis=0)\n",
        "    r_sq = 1 - ss_res / ss_tot\n",
        "    return r_sq\n",
        "\n",
        "def train(config):\n",
        "    # Load data\n",
        "    generator = GTExGenerator(pathway=config.pathway,\n",
        "                              batch_size=config.batch_size,\n",
        "                              m_low=config.m_low,\n",
        "                              m_high=config.m_high,\n",
        "                              inplace_mode=config.inplace_mode,\n",
        "                              random_seed=config.random_seed)\n",
        "\n",
        "    # Make model\n",
        "    # model = GAINGTEx(config.model)(x_dim=generator.nb_genes,\n",
        "    #                                 vocab_sizes=generator.vocab_sizes,\n",
        "    #                                 nb_numeric=generator.nb_numeric,\n",
        "    #                                 nb_categoric=generator.nb_categorical,\n",
        "    #                                 config=config)\n",
        "    model = GAINGTEx(x_dim=generator.nb_genes,\n",
        "                              vocab_sizes=generator.vocab_sizes,\n",
        "                              nb_numeric=generator.nb_numeric,\n",
        "                              nb_categoric=generator.nb_categorical,\n",
        "                              config=config)\n",
        "    opt = tf.keras.optimizers.Adam(config.lr['value'])\n",
        "\n",
        "    # For GAINGTEx only\n",
        "    disc_opt = tf.keras.optimizers.Adam(config.lr['value'])\n",
        "    opt = (opt, disc_opt)\n",
        "\n",
        "    model.compile(opt)\n",
        "\n",
        "    # Train model\n",
        "    early_stopper = tfk.callbacks.EarlyStopping(\n",
        "        monitor=\"val_loss\",\n",
        "        min_delta=0,\n",
        "        patience=config.patience['value'],\n",
        "        verbose=0,\n",
        "        mode=\"auto\",\n",
        "        baseline=None,\n",
        "        restore_best_weights=False,\n",
        "    )\n",
        "    alpha = 0.5\n",
        "    beta = 0.5\n",
        "    model.fit(generator.train_iterator_MCAR(alpha=alpha, beta=beta),\n",
        "              validation_data=generator.val_sample_MCAR(alpha=alpha, beta=beta),\n",
        "              epochs=20,\n",
        "              steps_per_epoch=10,\n",
        "              callbacks=[WandbCallback(), early_stopper])\n",
        "\n",
        "\n",
        "    # model.save(\n",
        "    #     '{}/checkpoints/{}_inplace{}_{}'.format(config.save_dir['value'], config.model['value'], config.inplace_mode['value'], config.pathway['value']))\n",
        "\n",
        "    return model, generator\n",
        "\n",
        "# Define command line arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--config', dest='config', default='configs/default_GTEx_inductive_imputation.yaml', type=str)\n",
        "parser.add_argument('--random_seed', dest='random_seed', default=0, type=int)\n",
        "args, unknown = parser.parse_known_args()\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project='GTEx_imputation', config=configGAIN)\n",
        "wandb.config.update({'random_seed': args.random_seed}, allow_val_change=True)\n",
        "config = wandb.config\n",
        "print(config)\n",
        "\n",
        "# Limit GPU\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"{}\".format(config.gpu)\n",
        "\n",
        "# Train model\n",
        "t = time.time()\n",
        "model, generator = train(config)\n",
        "t = (time.time() - t) / 3600\n",
        "\n",
        "# Save test loss\n",
        "x, _ = generator.test_sample_MCAR()\n",
        "x, cc, nc, mask = x\n",
        "if type(mask) is tuple:\n",
        "    mask = mask[0]\n",
        "x_obs = mask * x\n",
        "x_miss = (1 - mask) * x\n",
        "x_imp = model((x_obs, cc, nc, mask))\n",
        "\n",
        "# Calculating R^2 scores, which we will use to demonstrate the model's performance visually\n",
        "r2 = np.mean(r2_scores(x, x_imp, mask))\n",
        "\n",
        "# Save results\n",
        "name = '{}_inplace{}_{}'.format(config.model['value'], config.inplace_mode['value'], config.pathway['value'])\n",
        "with open(raw_data_dir + '/results/times_{}.txt'.format(name), 'a') as f:\n",
        "    f.write('{},'.format(t))\n",
        "with open(raw_data_dir + '/results/scores_{}.txt'.format(name), 'a') as f:\n",
        "    f.write('{},'.format(r2))\n",
        "\n",
        "print('Model: {}, Inplace: {}, Pathway: {}, Time: {}, R2: {}'\n",
        "      .format(config.model['value'], config.inplace_mode['value'], config.pathway['value'], t, r2))\n"
      ],
      "metadata": {
        "id": "9_bW6qPuIBJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "A key issue with running this model was that we were not privy to the pre-processed dataset that the original authors used, which included additional information that they did not release. Therefore, in order to ensure that the data was compatible with the model, we utilized a smaller dataset and sampled for additional data that we required.\n",
        "\n",
        "The paper we chose specifically uses R^2 to demonstrate how well the model has been able to predict the remainder of the transcriptome. The functions for this calculation are included in the code above. The ground truth transcriptome data x is compared to the predicted transcriptome data x_pred along with the mask generated for missing values.\n",
        "\n",
        "Upon running the model on the GAIN-GTEx model, we encountered R^2 values between -1 and -1.5. This range of values is highly unusual as it should  typically range from 0 to 1. Negative values suggest that the model performs worse than a model that simply predicts the mean of the response variable, indicating potential issues with the model's performance. It is difficult to judge the actual archictecture and performance of this model given that we had to perform a lot of data manipulation from our own knowledge of gene transciptomes and assumptions of their pre-processing steps.\n",
        "\n",
        "Our original hypothesis aimed to prove that the novel deep learning approaches proposed by the paper, both GAIN-GTEx and PMI, would outperform the other standard imputation models. This was shown to be true in the paper but since we were only able to run the data on the GAIN-GTEx model and received unreliable and suboptimal results, we cannot confirm this.\n",
        "\n",
        "#### Ablation Study\n",
        "\n",
        "We performed ablations by focusing on the layers of this particular model.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model comparison"
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We intended to compare the model above with median imputation and k-NN. Unfortunately, given that the data was incomplete without the pre-processing steps, we decided to only run it for the GAINGTEx model. Emphasizing the futility of running inadequate data through multiple models, this decision ensured that we could be more focused on meaningful evaluation of the selected model's performance."
      ],
      "metadata": {
        "id": "yuOpjgx5zd25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "The paper as of the current aims to be difficult to reproduce, primarily due to\n",
        "the fact that there exists a preprocessed dataset that the authors of the paper\n",
        "have preprocessed but we aren't privy to. This inturn, makes it quite\n",
        "challenging to formulate or replicate an accurate enough analysis that would be\n",
        "comparitive to the results in the paper. Testing on a smaller dataset while\n",
        "feasible lacks a large amount of the holistic tissue data that the model was\n",
        "able to process, and to build and train a modified dataset with the respective\n",
        "data within GTEx could take more weeks to perform properly. Additionally, it is\n",
        "worth noting that a large portion of the data is constantly updated and modified\n",
        "each year, so while their GAIN-GTEx and PMI models may have proven to be\n",
        "accurate, the same cant be said for different iterations of the data, or even\n",
        "the underlying tissue content.\n",
        "\n",
        "Understanding the models were\n",
        "relatively straightforward, especially as the author's provided diagrams of the archiectures, as well as incorporating available metadata for model training. The implementation of the model took more work as we needed to ensure that our matrices were compatible with the provided code.\n",
        "A suggestion we would make to the author, is to publish the pre-processed\n",
        "dataset that they worked on so that we may aim to reproduce their research in\n",
        "hopes of attainting similar results. In the next phase we would aim to\n",
        "methodically find a more effective way to pre-process the data ourselves such\n",
        "that we are able to train the model in an efficient and effective manner.\n"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1.   Viñas R, Azevedo T, Gamazon ER and Liò P (2021) Deep Learning Enables Fast and Accurate Imputation of Gene Expression. Front. Genet. 12:624128. doi: 10.3389/fgene.2021.624128\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "id": "cyXwNS_6lsW3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}